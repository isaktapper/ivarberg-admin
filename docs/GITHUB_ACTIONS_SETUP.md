# GitHub Actions Setup - Automatiserad Event Scraping

## Ã–versikt

Detta dokument beskriver hur du har migrerat dina event-scrapers frÃ¥n lokal/Vercel-kÃ¶rning till automatiserad schemalÃ¤ggning via GitHub Actions.

## ğŸ“ Filstruktur

```
/scripts/
  - run-scrapers.ts           # Standalone scraper-script

/.github/workflows/
  - daily-scraper.yml         # GitHub Actions workflow
```

## ğŸš€ Hur det fungerar

### 1. Standalone Script (`scripts/run-scrapers.ts`)

Ett fristÃ¥ende TypeScript-script som:
- HÃ¤mtar alla aktiva scrapers frÃ¥n registry
- KÃ¶r varje scraper sekventiellt
- Importerar events till Supabase
- Loggar detaljerad progress och resultat
- Hanterar fel gracefully (fortsÃ¤tter med nÃ¤sta scraper vid fel)

**FÃ¶rdelar mot API route:**
- Ingen Vercel timeout (kan kÃ¶ra hur lÃ¤nge som helst)
- Ingen kostnad fÃ¶r Vercel Pro
- BÃ¤ttre logging och debugging
- FullstÃ¤ndig kontroll Ã¶ver execution environment

### 2. GitHub Actions Workflow (`.github/workflows/daily-scraper.yml`)

**Trigger:**
- **Automatiskt**: Varje dag kl 06:00 svensk tid (05:00 UTC)
- **Manuellt**: Via GitHub Actions UI (workflow_dispatch)

**Steg:**
1. Checkar ut koden
2. SÃ¤tter upp Node.js 20
3. Installerar dependencies med `npm ci`
4. KÃ¶r scraper-scriptet med environment variables
5. Laddar upp logs vid fel
6. Skapar GitHub issue vid fel (endast om ingen Ã¶ppen issue finns)

## ğŸ” Environment Variables (GitHub Secrets)

### Obligatoriska secrets att lÃ¤gga till:

GÃ¥ till ditt GitHub repo â†’ **Settings** â†’ **Secrets and variables** â†’ **Actions** â†’ **New repository secret**

| Secret Name | VÃ¤rde | Beskrivning |
|-------------|-------|-------------|
| `NEXT_PUBLIC_SUPABASE_URL` | `https://xypvnwdfbhbsdcftzbvr.supabase.co` | Din Supabase projekt-URL |
| `SUPABASE_SERVICE_ROLE_KEY` | `eyJhbGc...` | Service role key frÃ¥n Supabase Dashboard |
| `OPENAI_API_KEY` | `sk-...` | OpenAI API key fÃ¶r AI-kategorisering |

### Hitta dina keys:

**Supabase:**
1. GÃ¥ till [Supabase Dashboard](https://app.supabase.com)
2. VÃ¤lj ditt projekt
3. GÃ¥ till **Settings** â†’ **API**
4. Kopiera:
   - `URL` â†’ `NEXT_PUBLIC_SUPABASE_URL`
   - `service_role` key â†’ `SUPABASE_SERVICE_ROLE_KEY`

**OpenAI:**
1. GÃ¥ till [OpenAI Platform](https://platform.openai.com/api-keys)
2. Skapa en ny API key
3. Kopiera â†’ `OPENAI_API_KEY`

## ğŸ§ª Testa lokalt

Innan du pushar till GitHub, testa att scriptet fungerar lokalt:

### 1. SÃ¤kerstÃ¤ll att du har rÃ¤tt env-variabler

Kontrollera att din `.env.local` innehÃ¥ller:
```env
NEXT_PUBLIC_SUPABASE_URL=https://xypvnwdfbhbsdcftzbvr.supabase.co
SUPABASE_SERVICE_ROLE_KEY=your_service_role_key
OPENAI_API_KEY=sk-your_openai_key
```

### 2. KÃ¶r scriptet

```bash
npx tsx --env-file=.env.local scripts/run-scrapers.ts
```

**FÃ¶rvÃ¤ntat resultat:**
```
ğŸš€ Starting iVarberg event scraping...

ğŸ“‹ Found 4 active scrapers

============================================================
ğŸ“¡ Running: Arena Varberg
ğŸ”— URL: https://arenavarberg.se/evenemang-varberg/
============================================================

  âœ“ Found 15 events (3.2s)

  ğŸ“Š Results for Arena Varberg:
     â€¢ Imported: 12 new events
     â€¢ Duplicates: 3 skipped
     â€¢ Import time: 8.5s

...

============================================================
ğŸ“Š FINAL SUMMARY
============================================================
âœ… Successfully scraped: 4/4 sources
ğŸ“¥ Total events found: 120
â• Total imported: 45
ğŸ”„ Total duplicates: 75
â±ï¸  Total time: 156.3s
============================================================
âœ… Scraping complete!
```

## ğŸ“¤ Deploy till GitHub

### 1. Pusha koden

```bash
git add .github/workflows/daily-scraper.yml
git add scripts/run-scrapers.ts
git add docs/GITHUB_ACTIONS_SETUP.md
git commit -m "Add GitHub Actions scraper workflow"
git push origin main
```

### 2. LÃ¤gg till secrets

GÃ¥ till ditt GitHub repo och lÃ¤gg till de 3 secrets som beskrivs ovan.

### 3. Verifiera workflow

1. GÃ¥ till **Actions**-fliken i ditt GitHub repo
2. Du ska se "Daily Event Scraper" workflow
3. Klicka pÃ¥ **Run workflow** â†’ **Run workflow** fÃ¶r att testa manuellt
4. VÃ¤nta ~3-6 minuter och kontrollera logs

## ğŸ“Š Ã–vervaka kÃ¶rningar

### GitHub Actions UI

Alla scraper-kÃ¶rningar loggas i GitHub Actions:
- GÃ¥ till **Actions**-fliken
- Klicka pÃ¥ en specifik kÃ¶rning fÃ¶r att se logs
- GrÃ¶n checkmark = success
- RÃ¶d X = failure (du fÃ¥r en GitHub issue)

### Supabase Logs

Alla kÃ¶rningar loggas ocksÃ¥ i din Supabase databas:
- GÃ¥ till din admin-panel â†’ `/scrapers`
- Se "KÃ¶rningshistorik" tabell
- Varje kÃ¶rning har:
  - Status (running, success, failed, partial)
  - Events found/imported/duplicates
  - Duration
  - Errors (om nÃ¥gra)

## ğŸ”§ FelsÃ¶kning

### "Missing environment variable"

**Problem:** Scriptet klagar pÃ¥ saknade env-variabler

**LÃ¶sning:**
1. Kontrollera att alla 3 secrets Ã¤r tillagda i GitHub
2. Verifiera att secret-namnen Ã¤r exakt rÃ¤tt (case-sensitive)
3. KÃ¶r workflow igen

### "All scrapers failed"

**Problem:** Alla scrapers failar

**MÃ¶jliga orsaker:**
1. **Supabase connection issue**
   - Kontrollera att `SUPABASE_SERVICE_ROLE_KEY` Ã¤r korrekt
   - Testa att connecta till Supabase frÃ¥n lokalt script
   
2. **Website structure changed**
   - En scraper-target kan ha Ã¤ndrat sin HTML-struktur
   - Kolla logs fÃ¶r specifik scraper som failar
   - Uppdatera CSS selectors i scraper-filen

3. **OpenAI rate limit**
   - FÃ¶r mÃ¥nga requests till OpenAI
   - VÃ¤nta en stund och kÃ¶r igen
   - Ã–ka delay i `aiCategorizer.ts` om det hÃ¤nder ofta

### "Timeout"

**Problem:** Workflow timeout efter 15 minuter

**LÃ¶sning:**
1. Ã–ka `timeout-minutes` i workflow-filen
2. Optimera scrapers (minska delay mellan requests)
3. Splitta upp i flera workflows (en per scraper)

### "No events found"

**Problem:** Scraper hittar 0 events

**MÃ¶jliga orsaker:**
1. **Inget fel** - det kan faktiskt inte finnas nÃ¥gra events
2. **HTML structure changed** - scraper hittar inte events lÃ¤ngre
3. **Rate limiting** - webbplatsen blockerar requests

**Debug:**
```bash
# Testa lokalt
npx tsx --env-file=.env.local scripts/run-scrapers.ts
```

Inspektera output och se vilken scraper som failar, sedan uppdatera den scraper-filen.

## ğŸ›ï¸ Konfigurera schema

FÃ¶r att Ã¤ndra nÃ¤r scraping kÃ¶rs, uppdatera cron-uttrycket i `.github/workflows/daily-scraper.yml`:

```yaml
on:
  schedule:
    - cron: '0 5 * * *'  # 06:00 svensk tid (UTC+1)
```

**Cron syntax:**
```
* * * * *
â”‚ â”‚ â”‚ â”‚ â”‚
â”‚ â”‚ â”‚ â”‚ â””â”€â”€â”€ Veckodag (0-6, sÃ¶ndag=0)
â”‚ â”‚ â”‚ â””â”€â”€â”€â”€â”€ MÃ¥nad (1-12)
â”‚ â”‚ â””â”€â”€â”€â”€â”€â”€â”€ Dag i mÃ¥nad (1-31)
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€ Timme (0-23)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Minut (0-59)
```

**Exempel:**
- `0 5 * * *` - Varje dag kl 05:00 UTC (06:00 svensk vintertid)
- `0 5,17 * * *` - TvÃ¥ gÃ¥nger per dag: 05:00 och 17:00 UTC
- `0 5 * * 1-5` - Vardagar kl 05:00 UTC (mÃ¥ndag-fredag)

**OBS:** AnvÃ¤nd alltid UTC-tid i cron! Sverige Ã¤r UTC+1 (vintertid) eller UTC+2 (sommartid).

## ğŸ“ˆ Prestanda

Baserat pÃ¥ nuvarande scrapers:

| Scraper | Genomsnittlig tid | Events per kÃ¶rning |
|---------|------------------|-------------------|
| Arena Varberg | ~30-60s | 15-30 |
| Varbergs Teater | ~20-40s | 10-20 |
| Visit Varberg | ~2-4 min | 100-200 |
| SocietÃ©n | ~30-60s | 10-20 |
| **Total** | **~3-6 min** | **150-250** |

**GitHub Actions free tier:**
- 2000 minuter/mÃ¥nad fÃ¶r privata repos
- Unlimited fÃ¶r publika repos
- Med 1 kÃ¶rning/dag = ~180 minuter/mÃ¥nad (vÃ¤l inom grÃ¤nsen!)

## ğŸ”„ UnderhÃ¥ll

### LÃ¤gga till ny scraper

1. Skapa ny scraper-klass i `/src/lib/scrapers/`
2. Registrera i `scraper-registry.ts`
3. Testa lokalt: `npx tsx scripts/run-scrapers.ts`
4. Pusha till GitHub - workflow kÃ¶r automatiskt nya scrapern

### Inaktivera scraper temporÃ¤rt

I `src/lib/scrapers/scraper-registry.ts`:
```typescript
{
  name: 'Arena Varberg',
  enabled: false,  // SÃ¤tt till false
  ...
}
```

### Uppdatera scraper-logik

1. Uppdatera scraper-filen (t.ex. `arena-varberg-scraper.ts`)
2. Testa lokalt
3. Commit och push
4. NÃ¤sta schemalagda kÃ¶rning anvÃ¤nder nya logiken

## ğŸ†˜ Support

Vid problem:

1. **Kolla GitHub Actions logs** - Detaljerad output frÃ¥n varje kÃ¶rning
2. **Kolla Supabase logs** - Admin panel â†’ `/scrapers`
3. **Testa lokalt** - `npx tsx scripts/run-scrapers.ts`
4. **Kolla GitHub Issues** - Automatiska issues skapas vid fel

## âœ… Checklista fÃ¶r setup

- [ ] `tsx` installerat (`npm install --save-dev tsx`)
- [ ] `scripts/run-scrapers.ts` skapad
- [ ] `.github/workflows/daily-scraper.yml` skapad
- [ ] Testat lokalt (`npx tsx scripts/run-scrapers.ts`)
- [ ] Pushad till GitHub
- [ ] GitHub secrets tillagda (alla 3)
- [ ] Manuell workflow-kÃ¶rning testad
- [ ] Verifierat att events importeras till Supabase

## ğŸ‰ Klart!

Din scraper kÃ¶r nu automatiskt varje dag kl 06:00 svensk tid! ğŸš€

Events importeras direkt till Supabase och du kan Ã¶vervaka allt frÃ¥n admin-panelen eller GitHub Actions.

